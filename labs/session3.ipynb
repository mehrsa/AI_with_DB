{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7308df8f",
   "metadata": {},
   "source": [
    "### Semantic Kernel Introduction\n",
    "\n",
    "#### Overview of Semantic Kernel (SK) and Its Importance\n",
    "***Semantic Kernel*** is an open-source SDK from Microsoft that acts as middleware between your application code and AI large language models (LLMs). It enables developers to easily integrate AI into apps by letting AI agents call code functions and by orchestrating complex tasks. SK is lightweight and modular, designed for enterprise-grade solutions with features like telemetry and filters for responsible AI. Major companies (including Microsoft) leverage SK because it’s flexible and future-proof – you can swap in new AI models as they emerge without rewriting your code. In short, SK helps build robust, scalable AI applications that can evolve with advancing AI capabilities.\n",
    "\n",
    "Key reasons why Semantic Kernel is important for AI application development:\n",
    "\n",
    "- Bridging AI and Code: **SK combines natural language prompts with your existing code and APIs**, allowing AI to take actions. The AI can request a function call and SK will execute that function and return results back to the model. This bridges the gap between what the AI intends and what your code can do.\n",
    "- **Plugins (Skills)**: You can expose functionalities (from simple math to complex business logic or external APIs) as SK plugins. By describing your code to the AI (via function definitions), the model can invoke these functions to fulfill user requests. This plugin architecture makes your AI solutions modular and extensible.\n",
    "- **Enterprise-ready**: SK includes support for security, observability, and compliance (e.g. integration with Azure services, monitoring, content filtering). Hooks and filters ensure you can enforce policies (for instance, prevent sensitive data leakage).\n",
    "- **Multi-modal & Future-Proof**: SK natively supports multiple AI services (OpenAI, Azure OpenAI, HuggingFace, etc.) and modalities. Chat-based APIs can be extended to voice or other modes. As new models (like vision-enabled models or better language models) come out, SK lets you plug them in without major changes.\n",
    "- **Rapid Development**: By handling the heavy lifting of prompt orchestration, function calling, and memory management, SK enables faster development of AI features. You focus on defining what you want the AI to do (skills, prompts) and SK handles how to do it. Microsoft claims that SK helps “deliver AI solutions faster than any other SDK” due to its ability to automatically call functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7f19c8",
   "metadata": {},
   "source": [
    "### Services and Core Components of SK\n",
    "Semantic Kernel's architecture revolves around a few core components and services:\n",
    "\n",
    "- **Kernel**: The central object that orchestrates everything. The Kernel holds configuration for AI services, manages plugins (skills), coordinates function calls, and maintains contextual state (memory). You typically create one Kernel instance in your app and use it to register functions and perform AI queries.\n",
    "- **AI Services**: SK connects to AI models for different tasks:\n",
    "Chat Models: e.g. Azure OpenAI GPT-4o-mini or GPT-4o for natural language generation and understanding.\n",
    "Embedding Models: for converting text to vector embeddings (used in memory/search).\n",
    "Other Modalities: connectors for images, speech, etc., if needed.\n",
    "\n",
    "Semantic Kernel can automatically read your .env (automatically read from root of project) to access Azure OpenAI using the following variables:\n",
    "\n",
    "    AZURE_OPENAI_ENDPOINT - The endpoint it should talk to by default\n",
    "    AZURE_OPENAI_API_KEY - The API Key it should use\n",
    "    AZURE_OPENAI_API_VERSION - Inference API version it should use per default\n",
    "    AZURE_OPENAI_CHAT_DEPLOYMENT_NAME - Model deployment name it should use per default\n",
    "\n",
    "Before you continue, make sure your .env (copied from .env.sample) is filled out correctly.\n",
    "\n",
    "You configure the Kernel with the endpoints/keys for the services you need. For example, adding an Azure OpenAI chat completion service as follows ->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8562d7aa",
   "metadata": {},
   "source": [
    "#### 1. Create a kernel and add a chat completion model to it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f940329",
   "metadata": {},
   "outputs": [
    {
     "ename": "ServiceInitializationError",
     "evalue": "chat_deployment_name is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mServiceInitializationError\u001b[39m                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m dotenv.load_dotenv()\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Auto-loads defaults from .env file, alternatively you can set endpoint, deployment_name and api_key directly\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m chat_completion=\u001b[43mAzureChatCompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m kernel.add_service(chat_completion)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/semantic_kernel/connectors/ai/open_ai/services/azure_chat_completion.py:99\u001b[39m, in \u001b[36mAzureChatCompletion.__init__\u001b[39m\u001b[34m(self, service_id, api_key, deployment_name, endpoint, base_url, api_version, ad_token, ad_token_provider, token_endpoint, default_headers, async_client, env_file_path, env_file_encoding, instruction_role)\u001b[39m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceInitializationError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to validate settings: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m azure_openai_settings.chat_deployment_name:\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceInitializationError(\u001b[33m\"\u001b[39m\u001b[33mchat_deployment_name is required.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    101\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m    102\u001b[39m     deployment_name=azure_openai_settings.chat_deployment_name,\n\u001b[32m    103\u001b[39m     endpoint=azure_openai_settings.endpoint,\n\u001b[32m   (...)\u001b[39m\u001b[32m    114\u001b[39m     instruction_role=instruction_role,\n\u001b[32m    115\u001b[39m )\n",
      "\u001b[31mServiceInitializationError\u001b[39m: chat_deployment_name is required."
     ]
    }
   ],
   "source": [
    "from semantic_kernel.kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "kernel = Kernel()\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Auto-loads defaults from .env file, alternatively you can set endpoint, deployment_name and api_key directly\n",
    "chat_completion=AzureChatCompletion()\n",
    "\n",
    "kernel.add_service(chat_completion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8250a28d",
   "metadata": {},
   "source": [
    "### 2. Provide the kernel with the required skills. These can be defined as \"functions\" and \"plugins\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3981a4b5",
   "metadata": {},
   "source": [
    "##### Define function via a prompt (natural language function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8188f41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Kernel is a lightweight, open-source development kit that lets you easily build AI agents and integrate the latest AI models into your codebase.\n",
      "\n",
      "Childish English Translation:  \n",
      "Semantic Kernel is a super cool, tiny tool you can use for free! It helps you make smart robot friends and put the newest AI magic into your own games and projects—easy peasy!\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"{{$input}}\\n\\ntranslate to {{$target_language}} and change the tone to {{$target_tone}}:\"\n",
    "\n",
    "rewrite_expert = kernel.add_function(\n",
    "    prompt=prompt_template,\n",
    "    function_name=\"toner\",\n",
    "    plugin_name=\"Toner\"\n",
    ")\n",
    "\n",
    "# Use the function\n",
    "sample_text = \"\"\"\n",
    "Semantic Kernel is a lightweight, open-source development kit that lets \n",
    "you easily build AI agents and integrate the latest AI models into your codebase\n",
    "\"\"\"\n",
    "\n",
    "# async function call need to be awaited\n",
    "summary = await kernel.invoke(rewrite_expert, input=sample_text, target_language = \"english\", target_tone=\"childish\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab0ed94",
   "metadata": {},
   "source": [
    "#### Define function via code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "401931bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List, Optional\n",
    "from semantic_kernel.functions import kernel_function\n",
    "# Define the LightModel and LightsPlugin classes\n",
    "\n",
    "class LightModel(TypedDict):\n",
    "    id: int\n",
    "    name: str\n",
    "    is_on: bool | None\n",
    "    brightness: int | None\n",
    "    hex: str | None\n",
    "\n",
    "\n",
    "class LightsPlugin:\n",
    "    def __init__(self, lights: list[LightModel]):\n",
    "        self.lights = lights\n",
    "\n",
    "    @kernel_function\n",
    "    async def get_lights(self) -> List[LightModel]:\n",
    "        \"\"\"Gets a list of lights and their current state.\"\"\"\n",
    "        return self.lights\n",
    "\n",
    "    @kernel_function\n",
    "    async def get_state(\n",
    "        self, id: Annotated[int, \"The ID of the light\"]\n",
    "    ) -> Optional[LightModel]:\n",
    "        \"\"\"Gets the state of a particular light.\"\"\"\n",
    "        for light in self.lights:\n",
    "            if light[\"id\"] == id:\n",
    "                return light\n",
    "        return None\n",
    "\n",
    "    @kernel_function\n",
    "    async def change_state(\n",
    "        self, id: Annotated[int, \"The ID of the light\"], new_state: LightModel\n",
    "    ) -> Optional[LightModel]:\n",
    "        \"\"\"Changes the state of the light.\"\"\"\n",
    "        for light in self.lights:\n",
    "            if light[\"id\"] == id:\n",
    "                light[\"is_on\"] = new_state.get(\"is_on\", light[\"is_on\"])\n",
    "                light[\"brightness\"] = new_state.get(\"brightness\", light[\"brightness\"])\n",
    "                light[\"hex\"] = new_state.get(\"hex\", light[\"hex\"])\n",
    "                return light\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21c916ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KernelPlugin(name='Lights', description=None, functions={'change_state': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='change_state', plugin_name='Lights', description='Changes the state of the light.', parameters=[KernelParameterMetadata(name='id', description='The ID of the light', default_value=None, type_='int', is_required=True, type_object=<class 'int'>, schema_data={'type': 'integer', 'description': 'The ID of the light'}, include_in_function_choices=True), KernelParameterMetadata(name='new_state', description=None, default_value=None, type_='LightModel', is_required=True, type_object=<class '__main__.LightModel'>, schema_data={'type': 'object', 'properties': {'id': {'type': 'integer'}, 'name': {'type': 'string'}, 'is_on': {'type': ['boolean', 'null']}, 'brightness': {'type': ['integer', 'null']}, 'hex': {'type': ['string', 'null']}}, 'required': ['id', 'name']}, include_in_function_choices=True)], is_prompt=False, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='LightModel', is_required=False, type_object=<class '__main__.LightModel'>, schema_data={'type': 'object', 'properties': {'id': {'type': 'integer'}, 'name': {'type': 'string'}, 'is_on': {'type': ['boolean', 'null']}, 'brightness': {'type': ['integer', 'null']}, 'hex': {'type': ['string', 'null']}}, 'required': ['id', 'name']}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x00000193689271D0>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x0000019368886A50>, method=<bound method LightsPlugin.change_state of <__main__.LightsPlugin object at 0x000001933ABE5510>>, stream_method=None), 'get_lights': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='get_lights', plugin_name='Lights', description='Gets a list of lights and their current state.', parameters=[], is_prompt=False, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='list[LightModel]', is_required=True, type_object=<class 'list'>, schema_data={'type': 'array'}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x0000019368927350>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x0000019368927410>, method=<bound method LightsPlugin.get_lights of <__main__.LightsPlugin object at 0x000001933ABE5510>>, stream_method=None), 'get_state': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='get_state', plugin_name='Lights', description='Gets the state of a particular light.', parameters=[KernelParameterMetadata(name='id', description='The ID of the light', default_value=None, type_='int', is_required=True, type_object=<class 'int'>, schema_data={'type': 'integer', 'description': 'The ID of the light'}, include_in_function_choices=True)], is_prompt=False, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='LightModel', is_required=False, type_object=<class '__main__.LightModel'>, schema_data={'type': 'object', 'properties': {'id': {'type': 'integer'}, 'name': {'type': 'string'}, 'is_on': {'type': ['boolean', 'null']}, 'brightness': {'type': ['integer', 'null']}, 'hex': {'type': ['string', 'null']}}, 'required': ['id', 'name']}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x00000193689278D0>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x0000019368927990>, method=<bound method LightsPlugin.get_state of <__main__.LightsPlugin object at 0x000001933ABE5510>>, stream_method=None)})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dependencies for the plugin\n",
    "# Example with toy data in memory\n",
    "from semantic_kernel.kernel import Kernel\n",
    "\n",
    "lights = [\n",
    "    {\"id\": 1, \"name\": \"Table Lamp\", \"is_on\": False, \"brightness\": 100, \"hex\": \"FF0000\"},\n",
    "    {\"id\": 2, \"name\": \"Porch light\", \"is_on\": False, \"brightness\": 50, \"hex\": \"00FF00\"},\n",
    "    {\"id\": 3, \"name\": \"Chandelier\", \"is_on\": True, \"brightness\": 75, \"hex\": \"0000FF\"},\n",
    "]\n",
    "\n",
    "plugin = LightsPlugin(lights=lights)\n",
    "kernel = Kernel()\n",
    "kernel.add_plugin(\n",
    "    plugin=plugin,\n",
    "    plugin_name=\"Lights\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d17d2a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant > The table lamp has been turned on. If you meant a different lamp, please specify which one you'd like to control.\n",
      "Assistant > All the lamps have been turned off. Here is the final state of all the lights:\n",
      "\n",
      "- Table Lamp: Off, Brightness 100, Color #FF0000\n",
      "- Porch light: Off, Brightness 50, Color #00FF00\n",
      "- Chandelier: Off, Brightness 75, Color #0000FF\n",
      "\n",
      "Let me know if you need anything else!\n",
      "Exiting the chat. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from semantic_kernel.connectors.ai.function_choice_behavior import (\n",
    "    FunctionChoiceBehavior,\n",
    ")\n",
    "from semantic_kernel.contents.chat_history import ChatHistory\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai.prompt_execution_settings.azure_chat_prompt_execution_settings import (\n",
    "    AzureChatPromptExecutionSettings)\n",
    "import logging\n",
    "\n",
    "# Enable  logging\n",
    "# logging.basicConfig(\n",
    "#     level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "# )\n",
    "\n",
    "# Enable planning\n",
    "execution_settings = AzureChatPromptExecutionSettings()\n",
    "execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "# Create a history of the conversation\n",
    "history = ChatHistory()\n",
    "user_message = \"Please turn on the lamp\"\n",
    "history.add_user_message(user_message)\n",
    "\n",
    "while(True):\n",
    "    # Get the response from the AI\n",
    "    \n",
    "    result = await chat_completion.get_chat_message_content(\n",
    "        chat_history=history,\n",
    "        settings=execution_settings,\n",
    "        kernel=kernel,\n",
    "    )\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Assistant > \" + str(result))\n",
    "    if(user_message.lower() == \"exit\"):\n",
    "        print(\"Exiting the chat. Goodbye!\")\n",
    "        break\n",
    "    # Add the message from the agent to the chat history\n",
    "    history.add_message(result)\n",
    "\n",
    "    # Get user input for the next message\n",
    "    user_message = input(\"Type another request or say 'exit' to end the chat > \")\n",
    "    history.add_user_message(user_message)\n",
    "    if( user_message.lower() == \"exit\"):\n",
    "        #autiomatic function calling\n",
    "        history.add_user_message(\"Please turn off all the lamps and give me final state of all the lights\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6533710c",
   "metadata": {},
   "source": [
    "### A real-world-like usecase: Contoso, Ltd. \n",
    "Let's assume that **Contoso, Ltd.** is an electronic devices distribution company that sells a large variety of devices (ex. headphones, laptops, TVs, etc.) new and refurbished.\n",
    "\n",
    "##### First, let's set up **the operational** database for Contoso, Ltd.\n",
    "- You need an Azure postgres db \n",
    "- Rename env.sample to .env and add credentials to .env file\n",
    "- You can run python db_inti.py to set up the database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c95fe8",
   "metadata": {},
   "source": [
    "#### **Example 1**: an inventory analyst working at the sales departments, wants to perform below actions based on current inventory and sales data of the company:\n",
    "- Get the most updated inventory for a given product name or id\n",
    "- A list of most sold product names for a given categroy\n",
    "- Add a new product to the database\n",
    "\n",
    "This data is stored in a postgreSQL database.\n",
    "\n",
    "In this example, we take a simple approach as follows:\n",
    "1. **securely** Connect to the database\n",
    "2. Load data from required tables from the database when needed\n",
    "3. Provide all necessary functions and plugins\n",
    "4. Develop a code using semantic kernel to get the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52883df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from src.get_conn import get_connection_uri\n",
    "from pandas import DataFrame\n",
    "\n",
    "\n",
    "\n",
    "class Contoso_DataPlugin:\n",
    "    def __init__(self, db_uri: str):\n",
    "        self.conn = psycopg2.connect(db_uri)\n",
    "        self.cursor = self.conn.cursor()\n",
    "        print(\"Connected to company's database successfully.\")\n",
    "\n",
    "    @kernel_function\n",
    "    async def get_product_info(self, product_name: Optional[str] = None, product_id: Optional[int] = None) -> list[dict]:\n",
    "        \"\"\"Gets all product information from the database given the name or ID of the product.\"\"\"\n",
    "        query = \"\"\"SELECT \n",
    "                    product_id,                   \n",
    "                    name,\n",
    "                    inventory,\n",
    "                    price,\n",
    "                    refurbished,\n",
    "                    category\n",
    "                FROM products\n",
    "                WHERE (LOWER(name) = LOWER(%(product_name)s) AND %(product_name)s IS NOT NULL)\n",
    "                   OR (product_id = %(product_id)s AND %(product_id)s IS NOT NULL)\n",
    "                   \"\"\"\n",
    "        if not product_name and not product_id:\n",
    "            print(\"No product name or ID provided.\")\n",
    "            return None\n",
    "        elif product_id:\n",
    "            self.cursor.execute(query, {\"product_name\": None, \"product_id\": product_id})\n",
    "        else:\n",
    "            self.cursor.execute(query, {\"product_name\": product_name, \"product_id\": None})\n",
    "\n",
    "            \n",
    "        rows = self.cursor.fetchall()\n",
    "        columns = [desc[0] for desc in self.cursor.description]\n",
    "        try:\n",
    "            products= DataFrame(rows, columns=columns)\n",
    "            products.to_dict(orient=\"records\")  # <-- JSON serializabl\n",
    "            \n",
    "            return products.to_dict(orient=\"records\")  # <-- JSON serializabl\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching product information: {e}\")\n",
    "            return None\n",
    "    @kernel_function\n",
    "    def most_sold_product(self, category: str) -> Optional[dict]:\n",
    "        \"\"\"Returns the most sold product in a given category.\"\"\"\n",
    "        query = \"\"\"\n",
    "            SELECT products.product_id, products.name, SUM(sales.quantity) AS total_sold\n",
    "            FROM sales\n",
    "            JOIN products ON sales.product_id = products.product_id\n",
    "            WHERE LOWER(products.category) = LOWER(%(category)s) \n",
    "            GROUP BY products.product_id, products.name\n",
    "            ORDER BY total_sold DESC\n",
    "            LIMIT 1;\n",
    "        \"\"\"\n",
    "        self.cursor.execute(query, {\"category\": category})\n",
    "        row = self.cursor.fetchone()\n",
    "        if row:\n",
    "            columns = [desc[0] for desc in self.cursor.description]\n",
    "            return dict(zip(columns, row))\n",
    "        else:\n",
    "            print(f\"No sales data found for category: {category}\")\n",
    "            return None\n",
    "    @kernel_function\n",
    "    async def add_product(self, name: str, description: str, price: float,\n",
    "                           inventory: int, refurbished: bool, category: str ) -> bool:\n",
    "        \"\"\"Adds a new product to the database.\"\"\"\n",
    "        query = \"\"\"INSERT INTO products (name, description, price, inventory, refurbished, category)\n",
    "                   VALUES (%(name)s, %(description)s, %(price)s, %(inventory)s, %(refurbished)s, %(category)s);\"\"\"  \n",
    "        try:\n",
    "            self.cursor.execute(\n",
    "                query,\n",
    "                {\n",
    "                    \"name\": name,\n",
    "                    \"description\": description,\n",
    "                    \"price\": price,\n",
    "                    \"inventory\": inventory,\n",
    "                    \"refurbished\": refurbished,\n",
    "                    \"category\": category\n",
    "                }\n",
    "            )\n",
    "            self.conn.commit()\n",
    "            print(f\"Product '{name}' added successfully.\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding product: {e}\")\n",
    "            self.conn.rollback()\n",
    "            return False\n",
    "\n",
    "    def close_connection(self):\n",
    "        \"\"\"Closes the database connection.\"\"\"\n",
    "        self.cursor.close()\n",
    "        self.conn.close()\n",
    "        print(\"Database connection closed.\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1fa594b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection uri was rertieved successfully.\n"
     ]
    }
   ],
   "source": [
    "conn_uri = get_connection_uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da043b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to company's database successfully.\n"
     ]
    }
   ],
   "source": [
    "chat_kernel = Kernel()\n",
    "\n",
    "# Auto-loads defaults from .env file, alternatively you can set endpoint, deployment_name and api_key directly\n",
    "chat_completion = AzureChatCompletion()\n",
    "chat_kernel.add_service(chat_completion)\n",
    "\n",
    "contoso_plugin = Contoso_DataPlugin(db_uri=conn_uri)\n",
    "\n",
    "chat_kernel.add_plugin(\n",
    "    plugin = contoso_plugin,\n",
    "    plugin_name=\"contoso_plugin\"\n",
    ") \n",
    "\n",
    "# Enable planning\n",
    "execution_settings = AzureChatPromptExecutionSettings()\n",
    "execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "# Create a history of the conversation\n",
    "history = ChatHistory()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb94c22",
   "metadata": {},
   "source": [
    "#### Example 1: asking a question about product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7015921c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant > The product with ID 15 is the \"Jabra Headphone Studio.\" Here are the details:\n",
      "\n",
      "- Inventory: 60 units available\n",
      "- Price: $179.99\n",
      "- Refurbished: No (this is a new product)\n",
      "- Category: Headphones\n",
      "\n",
      "Let me know if you need more information about this product or anything else!\n"
     ]
    }
   ],
   "source": [
    "user_message = \"Please give me information about the product with id 15\"\n",
    "history.add_user_message(user_message)\n",
    "\n",
    "# Get the response from the AI\n",
    "    \n",
    "result = await chat_completion.get_chat_message_content(\n",
    "    chat_history=history,\n",
    "    settings=execution_settings,\n",
    "    kernel=chat_kernel,\n",
    ")\n",
    "print(\"Assistant > \" + str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0da4e7",
   "metadata": {},
   "source": [
    "#### Example 2: adding a new product record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b04d7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product 'Lenovo Thinkpad' added successfully.\n",
      "Assistant > The product \"Lenovo Thinkpad\" with the description \"Latest model,\" priced at $699.99, inventory of 5, not refurbished, and in the \"Laptops\" category, has been successfully added to the database. If you need more details or want to manage this product, let me know!\n"
     ]
    }
   ],
   "source": [
    "user_message = \"add a new product with name 'Lenovo Thinkpad', description 'Latest model', price 699.99, inventory 5, refurbished False, category 'Laptops'\"\n",
    "history.add_user_message(user_message)\n",
    "result = await chat_completion.get_chat_message_content(\n",
    "    chat_history=history,\n",
    "    settings=execution_settings,\n",
    "    kernel=chat_kernel,\n",
    ")\n",
    "print(\"Assistant > \" + str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975c8932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
